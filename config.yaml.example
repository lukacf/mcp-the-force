# MCP Second-Brain Configuration Example
# Copy this to config.yaml and customize for your environment

mcp:
  host: 127.0.0.1
  port: 8000
  # Percentage of a model's total context window to fill with file content.
  # Files that fit within this limit are inlined directly in the prompt.
  # Files that exceed this limit are automatically uploaded to a vector store
  # for RAG (Retrieval Augmented Generation) via semantic search.
  # The remaining space (e.g., 15%) is reserved as a safety buffer for the prompt
  # template, tool definitions, and the model's generated response.
  context_percentage: 0.85
  default_temperature: 1.0  # Neutral temperature (0=deterministic, 2=creative)

logging:
  level: INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# Provider configuration
# API keys should go in secrets.yaml, not here
providers:
  openai:
    enabled: true
  vertex:
    enabled: false
    project: # your-gcp-project-id
    location: us-central1
  anthropic:
    enabled: false

# Session management
session:
  ttl_seconds: 3600  # 1 hour
  db_path: .mcp_sessions.sqlite3
  cleanup_probability: 0.01  # 1% chance to cleanup on each request

# Memory system
memory:
  enabled: true
  rollover_limit: 100  # Create new vector store after 100 items
  session_cutoff_hours: 24  # Look back 24 hours for related sessions
  summary_char_limit: 1000  # Max chars for conversation summaries
  max_files_per_commit: 20  # Max files to list in commit summaries

# Testing
adapter_mock: false  # Set to true to use mock adapters